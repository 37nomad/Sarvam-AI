{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10996911,"sourceType":"datasetVersion","datasetId":6845465},{"sourceId":11011760,"sourceType":"datasetVersion","datasetId":6856011},{"sourceId":11012075,"sourceType":"datasetVersion","datasetId":6856205},{"sourceId":11021657,"sourceType":"datasetVersion","datasetId":6863211}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"56f41aa8-544e-437d-ab38-ebce9a5b31a5","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:40:25.927922Z","iopub.execute_input":"2025-03-13T18:40:25.928150Z","iopub.status.idle":"2025-03-13T18:40:25.954566Z","shell.execute_reply.started":"2025-03-13T18:40:25.928131Z","shell.execute_reply":"2025-03-13T18:40:25.953791Z"}},"outputs":[],"execution_count":10},{"id":"4586c0f6-4dc7-4c68-89f6-520cbd0fdc2d","cell_type":"code","source":"image_names_finetuning = [d[\"image\"] for d in data]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:24:42.827266Z","iopub.execute_input":"2025-03-13T18:24:42.827777Z","iopub.status.idle":"2025-03-13T18:24:42.862270Z","shell.execute_reply.started":"2025-03-13T18:24:42.827749Z","shell.execute_reply":"2025-03-13T18:24:42.861177Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-0cb7db916166>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_names_finetuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"],"ename":"NameError","evalue":"name 'data' is not defined","output_type":"error"}],"execution_count":4},{"id":"273e5b8c-64ed-4e3d-9512-cb8738ebba1f","cell_type":"code","source":"import random\nimage_names_finetuning = random.sample(image_names_finetuning, 97)\nlen(image_names_finetuning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:24:44.459699Z","iopub.execute_input":"2025-03-13T18:24:44.460012Z","iopub.status.idle":"2025-03-13T18:24:44.470837Z","shell.execute_reply.started":"2025-03-13T18:24:44.459987Z","shell.execute_reply":"2025-03-13T18:24:44.469853Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ff7084f3329d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_names_finetuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_names_finetuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m97\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_names_finetuning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'image_names_finetuning' is not defined"],"ename":"NameError","evalue":"name 'image_names_finetuning' is not defined","output_type":"error"}],"execution_count":5},{"id":"4311f3f7-a9cc-4ea0-95c1-6cfe4be04c52","cell_type":"code","source":"image_names_test = [d[\"image\"] for d in data if d[\"image\"] not in image_names_finetuning]\nlen(image_names_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:24:44.998110Z","iopub.execute_input":"2025-03-13T18:24:44.998391Z","iopub.status.idle":"2025-03-13T18:24:45.009607Z","shell.execute_reply.started":"2025-03-13T18:24:44.998367Z","shell.execute_reply":"2025-03-13T18:24:45.008428Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-4d662781b801>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_names_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_names_finetuning\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_names_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"],"ename":"NameError","evalue":"name 'data' is not defined","output_type":"error"}],"execution_count":6},{"id":"d3c0336b-4b37-4814-8673-abb3bc434e7a","cell_type":"code","source":"import os\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:24:45.255514Z","iopub.execute_input":"2025-03-13T18:24:45.255749Z","iopub.status.idle":"2025-03-13T18:24:45.258957Z","shell.execute_reply.started":"2025-03-13T18:24:45.255730Z","shell.execute_reply":"2025-03-13T18:24:45.258369Z"}},"outputs":[],"execution_count":7},{"id":"b5ab4b60-d793-41c8-902b-2a7efe964086","cell_type":"code","source":"import os\nimport shutil\n\n# Paths\nsource_folder = \"/kaggle/input/images\"  # Folder where all images are stored\ndest_folder_1 = \"/kaggle/working/finetuning\"  # First new folder\ndest_folder_2 = \"/kaggle/working/test\"  # Second new folder\n\n# Ensure destination folders exist\nos.makedirs(dest_folder_1, exist_ok=True)\nos.makedirs(dest_folder_2, exist_ok=True)\n\n# Lists of selected images\nselected_images_1 = image_names_finetuning  # Modify as needed\nselected_images_2 = image_names_test  # Modify as needed\n\n# Function to copy images\ndef copy_images(image_list, destination):\n    for image_name in image_list:\n        source_path = os.path.join(source_folder, image_name)\n        dest_path = os.path.join(destination, image_name)\n        \n        if os.path.exists(source_path):  # Check if image exists in source\n            shutil.copy(source_path, dest_path)\n        else:\n            print(f\"Warning: {image_name} not found in {source_folder}\")\n\n# Copy images to respective folders\ncopy_images(selected_images_1, dest_folder_1)\ncopy_images(selected_images_2, dest_folder_2)\n\nprint(\"Images copied successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:24:45.524036Z","iopub.execute_input":"2025-03-13T18:24:45.524240Z","iopub.status.idle":"2025-03-13T18:24:45.535858Z","shell.execute_reply.started":"2025-03-13T18:24:45.524223Z","shell.execute_reply":"2025-03-13T18:24:45.534860Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-3ab741e27932>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Lists of selected images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mselected_images_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_names_finetuning\u001b[0m  \u001b[0;31m# Modify as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mselected_images_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_names_test\u001b[0m  \u001b[0;31m# Modify as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'image_names_finetuning' is not defined"],"ename":"NameError","evalue":"name 'image_names_finetuning' is not defined","output_type":"error"}],"execution_count":8},{"id":"7a8f6d9d-609c-43c0-bfe4-993e8929bf98","cell_type":"code","source":"import shutil\n\n# Paths of folders to be zipped\nfolder1 = \"/kaggle/working/finetuning\"\nfolder2 = \"/kaggle/working/test\"\n\n# Output zip file paths\nshutil.make_archive(\"folder1_backup\", 'zip', folder1)\nshutil.make_archive(\"folder2_backup\", 'zip', folder2)\n\nprint(\"Folders zipped successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:20:10.724507Z","iopub.execute_input":"2025-03-13T18:20:10.724789Z","iopub.status.idle":"2025-03-13T18:20:10.729782Z","shell.execute_reply.started":"2025-03-13T18:20:10.724767Z","shell.execute_reply":"2025-03-13T18:20:10.729161Z"}},"outputs":[{"name":"stdout","text":"Folders zipped successfully!\n","output_type":"stream"}],"execution_count":96},{"id":"a3e3a189-0dd6-4c37-a74f-c3a1f2bd1899","cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:20:10.948821Z","iopub.execute_input":"2025-03-13T18:20:10.949128Z","iopub.status.idle":"2025-03-13T18:20:12.238606Z","shell.execute_reply.started":"2025-03-13T18:20:10.949106Z","shell.execute_reply":"2025-03-13T18:20:12.237721Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nThe token `Llama-3.2-11B-Vision` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `Llama-3.2-11B-Vision`\n","output_type":"stream"}],"execution_count":97},{"id":"2653e01a-8db3-494f-87c7-23967b45c686","cell_type":"code","source":"import json\n\n# # Sample JSON data (list of dictionaries)\n# data = [\n#     {\"image\": \"image1.jpg\", \"text\": \"Extracted text from image1\"},\n#     {\"image\": \"image2.jpg\", \"text\": \"Extracted text from image2\"},\n#     {\"image\": \"image3.jpg\", \"text\": \"Extracted text from image3\"}\n# ]\n\n# Convert list of dictionaries to a single dictionary\nconverted_data = {d[\"image\"]: d[\"text\"] for d in data}\n\n# Save to a JSON file\njson_filename = \"ground_truth_cleaned.json\"\nwith open(json_filename, \"w\") as json_file:\n    json.dump(converted_data, json_file, indent=4)\n\nprint(f\"JSON file saved as {json_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:20:12.239952Z","iopub.execute_input":"2025-03-13T18:20:12.240259Z","iopub.status.idle":"2025-03-13T18:20:12.284607Z","shell.execute_reply.started":"2025-03-13T18:20:12.240235Z","shell.execute_reply":"2025-03-13T18:20:12.283243Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-98-0c0b43e2e45c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert list of dictionaries to a single dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mconverted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Save to a JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"],"ename":"NameError","evalue":"name 'data' is not defined","output_type":"error"}],"execution_count":98},{"id":"33569b04-3abe-4389-9705-94b943e626fa","cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\n\n# Load model and processor\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision\"\nmodel = MllamaForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:20:12.285033Z","iopub.status.idle":"2025-03-13T18:20:12.285282Z","shell.execute_reply":"2025-03-13T18:20:12.285178Z"}},"outputs":[],"execution_count":null},{"id":"422b972f-0f86-48a6-9d4e-a4ceee133934","cell_type":"code","source":"with open(\"/kaggle/input/ground-truth-text/ground_truth_cleaned.json\", \"r\") as file:\n    data = json.load(file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:20:12.286057Z","iopub.status.idle":"2025-03-13T18:20:12.286451Z","shell.execute_reply":"2025-03-13T18:20:12.286274Z"}},"outputs":[],"execution_count":null},{"id":"a34649cc-a0eb-4645-a286-c7167eabe37d","cell_type":"code","source":"# Path to your image\nimage_path = \"/kaggle/input/images/india_news_p000142.jpg\"\nimage_name = \"india_news_p000142.jpg\"\nground_truth = data[image_name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:20:12.287269Z","iopub.status.idle":"2025-03-13T18:20:12.287648Z","shell.execute_reply":"2025-03-13T18:20:12.287481Z"}},"outputs":[],"execution_count":null},{"id":"78cbe250-5bd5-4f82-93cc-bcba34b3c489","cell_type":"code","source":"# Function to perform OCR\ndef extract_text(image_path):\n    image = Image.open(image_path)\n    prompt = \"<|image|><|begin_of_text|>Extract all visible text from this image accurately:\"\n    replaced_prompt = \"Extract all visible text from this image accurately:\"\n    \n    inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(**inputs, max_new_tokens=900)  # Adjust max tokens if needed\n    \n    extracted_text = processor.decode(output[0], skip_special_tokens=True)\n    extracted_text = extracted_text.split(replaced_prompt)[-1].strip()\n    return extracted_text\n\n# Perform OCR on a single image\nextracted_text = extract_text(image_path)\n\n# Print extracted text and ground truth\nprint(\"Extracted Text:\", extracted_text)\nprint(\"Ground Truth:\", ground_truth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T23:46:26.427188Z","iopub.execute_input":"2025-03-12T23:46:26.427503Z","iopub.status.idle":"2025-03-12T23:48:35.566267Z","shell.execute_reply.started":"2025-03-12T23:46:26.427480Z","shell.execute_reply":"2025-03-12T23:48:35.565362Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Extracted Text: put in place a machinery for establishment of a mall box system to file patents and according exclusive marketing rights for 5 years. This provision was made in the Patents (Amendment) Act of 1999. Copyright protection in India India has one of the most modern copyright protection laws in the world. Major development in the area of copyright during 1999 was the amendment to the Copyright Act of 1957 to make it fully compatible with the provisions of the TRIPS Agreement. Called the Copyright (Amendment) Act, 1999, this amendment was signed by the President of India on December 30, 1999 and came into force on January 15, 2000. The earlier 1994 amendment to the Copyright Act of 1957 had provided protection to all original literary, dramatic, musical and artistic works, cinematography, films and sound recordings. It also brought sectors such as satellite broadcasting, computer software and digital technology under Indian copyright protection. The Copyright Act is now in full conformity with the TRIPS obligations. Concern has been expressed about the allegedly slow judicial system in India and the procedural issues involved in trial and conviction. The Indian judiciary is handling cases as expeditiously as possible. The year that has gone by has again witnessed the versatility of the impartial and independent interpretation of intellectual property rights, amplified by the tion of intellectual property rights, amplified by the gaps in the statute with the common sense of the common law. Indian enforcement agencies are working effectively and there is a decline in the levels of piracy in India. A summary of these measures is given below: 1. During the year the government continued to stress the need for strict enforcement of the Copyright Act and Rules. State governments and other Ministries were regularly requested to lay special attention to ensuring copyright protection in their functioning. The Government also brought out a Handbook of Copyright Law to create awareness about copyright right amongst the stakeholders, enforcement agencies, professional users like the scientific and academic communities and members of the public. Copies of the Handbook were circulated free of cost to the state and central government officials and police personnel and also provided to participants in various seminars and workshops on IPR matters held during the year. National Police Academy and National Academy of Customs, Excise and Narcotics conducted several training programs on copyright for the police and customs officers. Modules on copyright have been included in their regular training programs. The Department of Education, Ministry of Human Resource Development, Government of India has initiated several measures in the past for strengthening the enforcement of copyrights that include constitution of a Copyright Enforcement Advisory Council (CEAC), creation of separate cells in state police headquarters, encouraging setting up of collective administration societies and organization of seminars and workshops to create greater awareness about copyright law among the enforcement personnel and the general public. For collective administration of copyright, copyright societies have been set up for different classes of works. At present there are three registered copyright societies. These are the Society for Copyright Regulations (SCRIPT) for cinematography films, Indian Performing Rights Society Limited (IPRS) for musical works and Phonographic Performance Limited (PPL) for sound recordings. These societies, particularly the PPL and the IPRS, have been quite active in anti-piracy work. The PPL has even set up a special anti-piracy cell under a retired Director General of Police, and this cell has been working in tandem with the police. Several other measures to create general awareness about copyright and for encouraging study of intellectual property rights in the educational system, besides modernizing the Copyright Of- fice, are on the anvil. Consequent to the number of measures initiated by the government, there has been more activity in the enforcement of copyright laws in the country during the last year compared to previous years. As per the data relating to copyright offenses available with the National Crime Records Bureau, the number of copy- right cases registered has gone up from 479 in 1997 to 802 in 1998. The number of persons arrested has increased from 794 in 1997 to 980 in 1998. The value of seizures has gone up from Rs. 28.8 million in 1997 to Rs. 74.8 million in 1998. These figures reflect the general improvement in the enforcement of the copyright law. Digitized by Google Original from UNIVERSITY OF VIRGINIA. <OCR/> mall place machinery establishment system patents according rights years. the Patents (Amendment) Act made police pants various\nGround Truth: put in place a machinery for establishment of a\nmail box system to file patents and according\nexclusive marketing rights for 5 years. This pro-\nvision was made in the Patents (Amendment) Act\nof 1999.\n\nCopyright protection in India\n\nIndia has one of the most modern copyright protec-\ntion laws in the world. Major development in the\narea of copyright during 1999 was the amendment\nto the Copyright Act of 1957 to make it fully compat-\nible with the provisions of the TRIPS Agreement.\nCalled the Copyright (Amendment) Act, 1999, this\namendment was signed by the President of India on\nDecember 30, 1999 and came into force on January\n15, 2000.\n\nThe earlier 1994 amendment to the Copyright Act of\n1957 had provided protection to all original literary,\ndramatic, musical and artistic works, cinematogra-\nphy, films and sound recordings. It also brought\nsectors such as satellite broadcasting, computer soft-\nware and digital technology under Indian copyright\nprotection. The Copyright Act is now in full confor-\nmity with the TRIPS obligations.\n\nConcern has been expressed about the allegedly\nslow judicial system in India and the procedural\nissues involved in trial and conviction. The Indian\njudiciary is handling cases as expeditiously as pos-\nsible. The year that has gone by has again witnessed\nthe versatility of the impartial and independent In-\ndian judiciary when it comes to the issue of protec-\ntion of intellectual property rights, amplified by the\nencouraging trends with Indian courts plugging in\ngaps in the statute with the common sense of the\ncommon law.\n\nIndian enforcement agencies are working effectively\nand there is a decline in the levels of piracy in India.\nA summary of these measures is given below:\n\n1. During the year the government continued to stress\nthe need for strict enforcement of the Copyright\nAct and Rules. State governments and other Min-\nistries were regularly requested to lay special at-\ntention to ensuring copyright protection in their\nfunctioning.\n\n2. The Government also brought out a Handbook of\nCopyright Law to create awareness about copy-\nright amongst the stakeholders, enforcement agen-\ncies, professional users like the scientific and aca-\ndemic communities and members of the public.\nCopies of the Handbook were circulated free of\ncost to the state and central government officials\n\nand police personnel and also provided to partici-\npants in various seminars and workshops on IPR\nmatters held during the year.\n\n3. National Police Academy and National Academy\nof Customs, Excise and Narcotics conducted sev-\neral training programs on copyright for the police\nand customs officers. Modules on copyright have\nbeen included in their regular training programs.\n\n4. The Department of Education, Ministry of Human\nResource Development, Government of India has\ninitiated several measures in the past for strength-\nening the enforcement of copyrights that include\nconstitution of a Copyright Enforcement Advisory\nCouncil (CEAC), creation of separate cells in state\npolice headquarters, encouraging setting up of\ncollective administration societies and organiza-\ntion of seminars and workshops to create greater\nawareness about copyright law among the en-\nforcement personnel and the general public.\n\n5. For collective administration of copyright, copy-\nright societies have been set up for different classes\nof works. At present there are three registered\ncopyright societies. These are the Society for\nCopyright Regulations of Indian Producers of Films\n& Television (SCRIPT) for cinematography films,\nIndian Performing Rights Society Limited (IPRS)\nfor musical works and Phonographic Performance\nLimited (PPL) for sound recordings. These societ-\nies, particularly the PPL and the IPRS, have been\nquite active in anti-piracy work. The PPL has even\nset up a special anti-piracy cell under a retired\nDirector General of Police, and this cell has been\nworking in tandem with the police.\n\n6. Several other measures to create general aware-\nness about copyright and for encouraging study\nof intellectual property rights in the educational\nsystem, besides modernizing the Copyright Of-\nfice, are on the anvil.\n\nConsequent to the number of measures initiated by\nthe government, there has been more activity in the\nenforcement of copyright laws in the country during\nthe last year compared to previous years. As per the\ndata relating to copyright offenses available with the\nNational Crime Records Bureau, the number of copy-\nright cases registered has gone up from 479 in 1997\nto 802 in 1998. The number of persons arrested has\nincreased from 794 in 1997 to 980 in 1998. The\nvalue of seizures has gone up from Rs. 28.8 million\nin 1997 to Rs.74.8 million in 1998. These figures\nreflect the general improvement in the enforcement\nof the copyright law.\n\n \n\nGoogle\n\n \n\n17\n\f\n","output_type":"stream"}],"execution_count":67},{"id":"80ab7727-0a51-4f0c-afd0-2777c5c8f1f5","cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:21:45.118904Z","iopub.execute_input":"2025-03-13T18:21:45.119115Z","iopub.status.idle":"2025-03-13T18:21:49.959274Z","shell.execute_reply.started":"2025-03-13T18:21:45.119096Z","shell.execute_reply":"2025-03-13T18:21:49.958347Z"}},"outputs":[],"execution_count":2},{"id":"833401e0-f4cb-47b0-9891-8e8a82ef4c29","cell_type":"code","source":"# Paths\ntest_images_folder = \"/kaggle/input/test-dataset\"  # Folder containing test images\nground_truth_json = \"/kaggle/input/ground-truth-text/ground_truth_cleaned.json\"  # JSON file with image-text mapping\n\n# Load ground truth data\nwith open(ground_truth_json, \"r\") as f:\n    ground_truth_data = json.load(f)  # Expecting format: [{\"image\": \"image_name.jpg\", \"text\": \"ground truth text\"}, ...]\n\n# Convert ground truth data to a dictionary for easy lookup\n# ground_truth_dict = {item[\"image\"]: item[\"text\"] for item in ground_truth_data}\nground_truth_dict = ground_truth_data\n\n# Function to extract text from image\ndef extract_text(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    prompt = \"<|image|><|begin_of_text|>Extract all visible text from this image accurately:\"\n    replaced_prompt = \"Extract all visible text from this image accurately:\"\n    \n    inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n    output = model.generate(**inputs, max_new_tokens=900)\n    \n    extracted_text = processor.decode(output[0], skip_special_tokens=True)\n    extracted_text = extracted_text.split(replaced_prompt)[-1].strip()\n    return extracted_text\n\n# Evaluate WER & CER\nwer_scores, cer_scores = [], []\nimage_files = os.listdir(test_images_folder)  # Select only the first 2 images\n\n# Initialize progress bar\nprogress_bar = tqdm(total=len(image_files), desc=\"Processing Images\", unit=\"image\")\n\nfor image_name in tqdm(image_files):\n    image_path = os.path.join(test_images_folder, image_name)\n\n    # Retrieve ground truth text\n    ground_truth_text = ground_truth_dict.get(image_name, \"\")\n\n    # If ground truth is missing, skip this image\n    if not ground_truth_text:\n        print(f\"Skipping {image_name}: No ground truth found.\")\n        continue\n\n    # Extract text using the model\n    predicted_text = extract_text(image_path)\n\n    # Compute WER & CER\n    wer_scores.append(wer(ground_truth_text, predicted_text))\n    cer_scores.append(cer(ground_truth_text, predicted_text))\n\n    progress_bar.update(1)\n\nprogress_bar.close()\n\n# Compute final scores\naverage_wer = sum(wer_scores) / len(wer_scores)\naverage_cer = sum(cer_scores) / len(cer_scores)\n\nprint(f\"Average WER: {average_wer:.4f}\")\nprint(f\"Average CER: {average_cer:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:18:11.421015Z","iopub.execute_input":"2025-03-13T11:18:11.421382Z","iopub.status.idle":"2025-03-13T12:27:11.901696Z","shell.execute_reply.started":"2025-03-13T11:18:11.421352Z","shell.execute_reply":"2025-03-13T12:27:11.901010Z"}},"outputs":[{"name":"stderr","text":"\nProcessing Images:   0%|          | 0/2 [08:33<?, ?image/s]]\u001b[A\n  0%|          | 0/40 [00:00<?, ?it/s]\n  2%|â–Ž         | 1/40 [01:44<1:07:50, 104.37s/it]:07:50, 104.37s/image]\u001b[A\n  5%|â–Œ         | 2/40 [03:27<1:05:48, 103.91s/it]:05:48, 103.91s/image]\u001b[A\n  8%|â–Š         | 3/40 [05:11<1:03:57, 103.71s/it]:03:57, 103.71s/image]\u001b[A\n 10%|â–ˆ         | 4/40 [06:54<1:02:11, 103.64s/it]:02:11, 103.64s/image]\u001b[A\n 12%|â–ˆâ–Ž        | 5/40 [08:38<1:00:26, 103.63s/it]:00:26, 103.63s/image]\u001b[A\n 15%|â–ˆâ–Œ        | 6/40 [10:22<58:42, 103.59s/it]  8:42, 103.59s/image]  \u001b[A\n 18%|â–ˆâ–Š        | 7/40 [12:05<56:57, 103.57s/it]<56:57, 103.57s/image]\u001b[A\n 20%|â–ˆâ–ˆ        | 8/40 [13:49<55:14, 103.56s/it]<55:14, 103.56s/image]\u001b[A\n 22%|â–ˆâ–ˆâ–Ž       | 9/40 [15:32<53:29, 103.52s/it]<53:29, 103.52s/image]\u001b[A\n 25%|â–ˆâ–ˆâ–Œ       | 10/40 [17:15<51:44, 103.47s/it]<51:44, 103.47s/image]\u001b[A\n 28%|â–ˆâ–ˆâ–Š       | 11/40 [18:59<50:00, 103.45s/it]<50:00, 103.45s/image]\u001b[A\n 30%|â–ˆâ–ˆâ–ˆ       | 12/40 [20:42<48:17, 103.47s/it]<48:17, 103.47s/image]\u001b[A\n 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [22:26<46:33, 103.45s/it]<46:33, 103.45s/image]\u001b[A\n 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [24:09<44:48, 103.42s/it]<44:48, 103.42s/image]\u001b[A\n 38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [25:53<43:05, 103.43s/it]<43:05, 103.43s/image]\u001b[A\n 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [27:36<41:24, 103.50s/it]<41:24, 103.50s/image]\u001b[A\n 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [29:20<39:41, 103.55s/it]<39:41, 103.55s/image]\u001b[A\n 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [31:03<37:58, 103.55s/it]<37:58, 103.55s/image]\u001b[A\n 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [32:47<36:15, 103.58s/it]<36:15, 103.58s/image]\u001b[A\n 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [34:31<34:31, 103.55s/it]<34:31, 103.55s/image]\u001b[A\n 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 21/40 [36:14<32:47, 103.54s/it]<32:47, 103.54s/image]\u001b[A\n 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [37:58<31:03, 103.53s/it]<31:03, 103.53s/image]\u001b[A\n 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [41:25<27:36, 103.55s/it]<27:36, 103.55s/image]\u001b[A\n 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [43:08<25:53, 103.53s/it]<25:53, 103.53s/image]\u001b[A\n 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 26/40 [44:52<24:09, 103.55s/it]<24:09, 103.55s/image]\u001b[A\n 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [46:35<22:25, 103.52s/it]<22:25, 103.52s/image]\u001b[A\n 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 28/40 [48:19<20:41, 103.49s/it]<20:41, 103.49s/image]\u001b[A\n 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [50:02<18:58, 103.50s/it]<18:58, 103.50s/image]\u001b[A\n 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [51:46<17:15, 103.51s/it]<17:15, 103.51s/image]\u001b[A\n 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [53:29<15:31, 103.52s/it]<15:31, 103.52s/image]\u001b[A\n 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [55:13<13:48, 103.51s/it]<13:48, 103.51s/image]\u001b[A\n 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [56:56<12:04, 103.46s/it]<12:04, 103.46s/image]\u001b[A\n 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 34/40 [58:40<10:20, 103.45s/it]<10:20, 103.45s/image]\u001b[A\n 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [1:00:23<08:37, 103.45s/it]<08:37, 103.45s/image]\u001b[A\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [1:02:06<06:53, 103.43s/it]<06:53, 103.43s/image]\u001b[A\n 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 37/40 [1:03:50<05:10, 103.42s/it]<05:10, 103.42s/image]\u001b[A\n 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [1:05:33<03:26, 103.41s/it]<03:26, 103.41s/image]\u001b[A\n 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 39/40 [1:07:17<01:43, 103.41s/it]<01:43, 103.41s/image]\u001b[A\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [1:09:00<00:00, 103.51s/it]<00:00, 103.39s/image]\u001b[A\nProcessing Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [1:09:00<00:00, 103.51s/image]","output_type":"stream"},{"name":"stdout","text":"Average WER: 2.2440\nAverage CER: 2.0135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"id":"e9a5c28c-01f2-4175-85f2-f37095824719","cell_type":"code","source":"wer_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:31:33.045047Z","iopub.execute_input":"2025-03-13T12:31:33.045384Z","iopub.status.idle":"2025-03-13T12:31:33.050685Z","shell.execute_reply.started":"2025-03-13T12:31:33.045355Z","shell.execute_reply":"2025-03-13T12:31:33.049893Z"},"scrolled":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[0.7518072289156627,\n 0.7033707865168539,\n 0.39361702127659576,\n 0.4772727272727273,\n 0.45077720207253885,\n 0.3729903536977492,\n 0.853904282115869,\n 0.8076285240464345,\n 0.5024154589371981,\n 2.65,\n 0.9528985507246377,\n 0.6453900709219859,\n 0.5962059620596206,\n 0.7193675889328063,\n 0.8324607329842932,\n 0.9277456647398844,\n 0.5731958762886598,\n 0.5721830985915493,\n 0.5871559633027523,\n 2.0727969348659006,\n 0.46153846153846156,\n 0.2253922967189729,\n 0.5863267670915412,\n 0.6684397163120568,\n 1.583710407239819,\n 0.6673114119922631,\n 0.31490015360983103,\n 0.3746928746928747,\n 0.43451776649746193,\n 0.44411326378539495,\n 0.4294385432473445,\n 0.9401041666666666,\n 0.34449093444909346,\n 0.35419440745672437,\n 0.7054409005628518,\n 0.5396419437340153,\n 0.9267326732673268,\n 0.6095791001451378,\n 62.22222222222222,\n 0.4846743295019157]"},"metadata":{}}],"execution_count":11},{"id":"53da2c25-71f3-482a-a451-94f92efc0e83","cell_type":"code","source":"cer_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:31:48.889972Z","iopub.execute_input":"2025-03-13T12:31:48.890253Z","iopub.status.idle":"2025-03-13T12:31:48.895542Z","shell.execute_reply.started":"2025-03-13T12:31:48.890231Z","shell.execute_reply":"2025-03-13T12:31:48.894725Z"},"scrolled":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[0.5530329904221355,\n 0.4834289356277884,\n 0.2672086720867209,\n 0.36010620643876534,\n 0.08818635607321132,\n 0.1719446399249355,\n 0.7372145384367964,\n 0.6281227694503926,\n 0.2770448548812665,\n 2.551699716713881,\n 0.899641577060932,\n 0.5277777777777778,\n 0.4519309778142975,\n 0.4860568878973787,\n 0.7503579952267303,\n 0.8404255319148937,\n 0.3322188449848024,\n 0.42442008666836606,\n 0.278264497288277,\n 1.7348916761687572,\n 0.12179930795847752,\n 0.036885245901639344,\n 0.4247472856608012,\n 0.45392749244712993,\n 1.1933911159263273,\n 0.4442328618063112,\n 0.12578488960907433,\n 0.18241563055062168,\n 0.2983561222922108,\n 0.10186153524367288,\n 0.08379888268156424,\n 0.8081639803784163,\n 0.13385986229242608,\n 0.1568663257852447,\n 0.5457446808510639,\n 0.4575692963752665,\n 0.7584783249778826,\n 0.32176105508145847,\n 60.64102564102564,\n 0.40563056592933067]"},"metadata":{}}],"execution_count":12},{"id":"23485733-f94d-4c89-860d-bb77419c6fb1","cell_type":"markdown","source":"# Fine tuning phase ","metadata":{}},{"id":"63ad391c-5f34-4b38-90b1-5a4a5e49a540","cell_type":"code","source":"import torch\n# import bitsandbytes as bnb|\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom transformers import DataCollatorForSeq2Seq, default_data_collator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T15:48:11.452097Z","iopub.execute_input":"2025-03-14T15:48:11.452433Z","iopub.status.idle":"2025-03-14T15:48:31.712775Z","shell.execute_reply.started":"2025-03-14T15:48:11.452406Z","shell.execute_reply":"2025-03-14T15:48:31.711878Z"}},"outputs":[],"execution_count":2},{"id":"483ae79c-5613-4bef-9305-b327581d7deb","cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T15:48:45.374981Z","iopub.execute_input":"2025-03-14T15:48:45.375650Z","iopub.status.idle":"2025-03-14T15:48:46.604750Z","shell.execute_reply.started":"2025-03-14T15:48:45.375618Z","shell.execute_reply":"2025-03-14T15:48:46.603691Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nThe token `Llama-3.2-11B-Vision` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `Llama-3.2-11B-Vision`\n","output_type":"stream"}],"execution_count":3},{"id":"53103b57-e0d5-4c0c-8c75-ca2ce76727b5","cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"ðŸ”¥ CUDA is available! Checking memory usage...\")\n\n    # Total memory in the GPU\n    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n    # Currently allocated memory by PyTorch\n    allocated_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n\n    # Reserved memory by PyTorch (including cached memory)\n    reserved_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n\n    # Free memory available (approximation)\n    free_memory = total_memory - allocated_memory\n\n    print(f\"ðŸ›  Total GPU Memory: {total_memory:.2f} GB\")\n    print(f\"ðŸ’¾ Allocated Memory: {allocated_memory:.2f} GB\")\n    print(f\"ðŸ“¦ Reserved Memory: {reserved_memory:.2f} GB\")\n    print(f\"âœ… Free Memory: {free_memory:.2f} GB\")\nelse:\n    print(\"âŒ No CUDA device found. Running on CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T15:48:51.231600Z","iopub.execute_input":"2025-03-14T15:48:51.232013Z","iopub.status.idle":"2025-03-14T15:48:51.359201Z","shell.execute_reply.started":"2025-03-14T15:48:51.231977Z","shell.execute_reply":"2025-03-14T15:48:51.358535Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¥ CUDA is available! Checking memory usage...\nðŸ›  Total GPU Memory: 14.74 GB\nðŸ’¾ Allocated Memory: 0.00 GB\nðŸ“¦ Reserved Memory: 0.00 GB\nâœ… Free Memory: 14.74 GB\n","output_type":"stream"}],"execution_count":4},{"id":"ee3bda13-4e50-4b51-b003-4067c5cac45b","cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  \n    bnb_4bit_compute_dtype=torch.float16,  \n    bnb_4bit_use_double_quant=True,  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:18:22.417484Z","iopub.execute_input":"2025-03-14T16:18:22.417828Z","iopub.status.idle":"2025-03-14T16:18:22.438659Z","shell.execute_reply.started":"2025-03-14T16:18:22.417801Z","shell.execute_reply":"2025-03-14T16:18:22.437819Z"}},"outputs":[],"execution_count":45},{"id":"8b1101a4-8fc8-45b0-bd58-6814e455b900","cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:19:25.687556Z","iopub.execute_input":"2025-03-14T16:19:25.688030Z","iopub.status.idle":"2025-03-14T16:19:29.433376Z","shell.execute_reply.started":"2025-03-14T16:19:25.687992Z","shell.execute_reply":"2025-03-14T16:19:29.432192Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":47},{"id":"355acc9b-3887-42db-8ab2-fd7270b91d71","cell_type":"code","source":"import requests\nimport torch\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\n\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision\"\n\nmodel = MllamaForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\n\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:19:39.663660Z","iopub.execute_input":"2025-03-14T16:19:39.663957Z","iopub.status.idle":"2025-03-14T16:19:40.236932Z","shell.execute_reply.started":"2025-03-14T16:19:39.663936Z","shell.execute_reply":"2025-03-14T16:19:40.235674Z"},"scrolled":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-37f090a97695>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Llama-3.2-11B-Vision\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = MllamaForConditionalGeneration.from_pretrained(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3669\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3670\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3671\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"],"ename":"ImportError","evalue":"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`","output_type":"error"}],"execution_count":48},{"id":"02d308b0-9962-4e4a-9f44-890d717f108f","cell_type":"code","source":"import os\nimport json\n\n# Define paths\nimage_folder = \"/kaggle/input/finetuning-dataset\"  # Change to your actual folder path\njson_file = \"/kaggle/input/ground-truth-text/ground_truth_cleaned.json\"  # JSON file containing completion mappings\n\n# Define a fixed prompt\nprompt_text = \"<|image|><|begin_of_text|>Extract all visible text from this image accurately:\"\n\n# Load JSON file with completions\nwith open(json_file, \"r\") as f:\n    completion_data = json.load(f)  # Dictionary where keys are image filenames\n\n# Get all image paths and extract corresponding completion\ndata = {\"image_path\": [], \"prompt\": [], \"completion\": []}\n\nfor image_name in os.listdir(image_folder):\n    if image_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Ensure only images are processed\n        image_path = os.path.join(image_folder, image_name)\n        if completion_data.get(image_name, \"No description available\") != \"No description available\":\n            completion = completion_data.get(image_name, \"No description available\")\n        else :\n            print(f\"{image_name} has no description available\")\n        # completion = completion_data.get(image_name, \"No description available\") # Default if not found\n\n        # Append to dictionary\n        data[\"image_path\"].append(image_path)\n        data[\"prompt\"].append(prompt_text)\n        data[\"completion\"].append(completion)\n\n# Print final dataset\nprint(data.keys())\nprint(len(data[\"image_path\"]))\nprint(len(data[\"prompt\"]))\nprint(len(data[\"completion\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:34.951642Z","iopub.execute_input":"2025-03-14T16:03:34.952029Z","iopub.status.idle":"2025-03-14T16:03:35.003723Z","shell.execute_reply.started":"2025-03-14T16:03:34.951998Z","shell.execute_reply":"2025-03-14T16:03:35.002755Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['image_path', 'prompt', 'completion'])\n97\n97\n97\n","output_type":"stream"}],"execution_count":6},{"id":"12adadf1-a9bb-4965-b58c-19caf43de8c3","cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom PIL import Image\nimport torch\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:38.528691Z","iopub.execute_input":"2025-03-14T16:03:38.528988Z","iopub.status.idle":"2025-03-14T16:03:38.533028Z","shell.execute_reply.started":"2025-03-14T16:03:38.528965Z","shell.execute_reply":"2025-03-14T16:03:38.532028Z"}},"outputs":[],"execution_count":7},{"id":"3d01ddf1-a0d4-409a-908d-48d6f7498dda","cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(data)\n# df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:39.211520Z","iopub.execute_input":"2025-03-14T16:03:39.211935Z","iopub.status.idle":"2025-03-14T16:03:39.226318Z","shell.execute_reply.started":"2025-03-14T16:03:39.211889Z","shell.execute_reply":"2025-03-14T16:03:39.225509Z"}},"outputs":[],"execution_count":8},{"id":"71e15447-bc8e-46ae-904e-c7d20c87fd54","cell_type":"code","source":"# Step 2: Convert to a Hugging Face Dataset\nfrom datasets import Dataset\nraw_dataset = Dataset.from_pandas(df)\nprint(raw_dataset)\nprint(type(raw_dataset))\nprint(raw_dataset['prompt'][0])\nprint(raw_dataset['image_path'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:41.576484Z","iopub.execute_input":"2025-03-14T16:03:41.576818Z","iopub.status.idle":"2025-03-14T16:03:41.638087Z","shell.execute_reply.started":"2025-03-14T16:03:41.576790Z","shell.execute_reply":"2025-03-14T16:03:41.637372Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['image_path', 'prompt', 'completion'],\n    num_rows: 97\n})\n<class 'datasets.arrow_dataset.Dataset'>\n<|image|><|begin_of_text|>Extract all visible text from this image accurately:\n/kaggle/input/finetuning-dataset/india_news_p000069.jpg\n","output_type":"stream"}],"execution_count":9},{"id":"527d48b0-59c7-49a8-a460-60397bc31f5d","cell_type":"code","source":"model_inputs = processor(\n        images=Image.open(raw_dataset['image_path'][0]).convert('RGB'),\n        text=raw_dataset['prompt'][0],\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\nprint(model_inputs.keys())\nprint(model_inputs[\"input_ids\"][0])\nprint(model_inputs[\"pixel_values\"].shape)  # Image shape\nprint(model_inputs[\"input_ids\"].shape)  # Tokenized text shape\nprint(model_inputs[\"attention_mask\"].shape)  # Attention mask shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:43.684795Z","iopub.execute_input":"2025-03-14T16:03:43.685115Z","iopub.status.idle":"2025-03-14T16:03:43.957196Z","shell.execute_reply.started":"2025-03-14T16:03:43.685091Z","shell.execute_reply":"2025-03-14T16:03:43.956416Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\ntensor([128000, 128256, 128000,  30059,    682,   9621,   1495,    505,    420,\n          2217,  30357,     25])\ntorch.Size([1, 1, 4, 3, 448, 448])\ntorch.Size([1, 12])\ntorch.Size([1, 12])\n","output_type":"stream"}],"execution_count":10},{"id":"278167ef-4d3e-4551-9abd-d5d8d0713c5d","cell_type":"code","source":"labels = processor(\n        images=None,\n        text=raw_dataset[\"prompt\"][25].replace(\"<|image|>\",\"\"),\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\nlabels2 = processor(\n        images=None,\n        text=raw_dataset[\"prompt\"][0].replace(\"<|image|>\",\"\"),\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\nprint(labels)\nprint(labels.keys())\nprint(labels.input_ids == labels2.input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:45.910773Z","iopub.execute_input":"2025-03-14T16:03:45.911209Z","iopub.status.idle":"2025-03-14T16:03:45.923015Z","shell.execute_reply.started":"2025-03-14T16:03:45.911174Z","shell.execute_reply":"2025-03-14T16:03:45.921922Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[128000, 128000,  30059,    682,   9621,   1495,    505,    420,   2217,\n          30357,     25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\ndict_keys(['input_ids', 'attention_mask'])\ntensor([[True, True, True, True, True, True, True, True, True, True, True]])\n","output_type":"stream"}],"execution_count":11},{"id":"8d01cd57-b948-4b60-af4c-c7122ce8239c","cell_type":"code","source":"prompt_tokens = processor(images=None,text=raw_dataset[\"prompt\"][0].replace(\"<|image|>\",\"\"), return_tensors=\"pt\").input_ids[0]\nprint(prompt_tokens)\nprint(len(prompt_tokens))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:48.096797Z","iopub.execute_input":"2025-03-14T16:03:48.097146Z","iopub.status.idle":"2025-03-14T16:03:48.104386Z","shell.execute_reply.started":"2025-03-14T16:03:48.097119Z","shell.execute_reply":"2025-03-14T16:03:48.103366Z"}},"outputs":[{"name":"stdout","text":"tensor([128000, 128000,  30059,    682,   9621,   1495,    505,    420,   2217,\n         30357,     25])\n11\n","output_type":"stream"}],"execution_count":12},{"id":"fe2078b4-077c-4066-b51f-f9df6aee6af5","cell_type":"code","source":"model_inputs = processor(\n        images=Image.open(raw_dataset[\"image_path\"][0]).convert(\"RGB\"),\n        text=raw_dataset['prompt'][0],\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\nprint(model_inputs.keys())\n# print(len(model_inputs.keys))\nprint(\"input_ids shape: \\n\", model_inputs[\"input_ids\"].shape)\nprint(\"attention_mask shape: \\n\", model_inputs[\"attention_mask\"].shape)\nprint(\"pixel_values shape: \\n\", model_inputs[\"pixel_values\"].shape)\nprint(\"aspect_ratio_ids shape: \\n\", model_inputs[\"aspect_ratio_ids\"].shape)\nprint(\"aspect_ratio_mask shape: \\n\", model_inputs[\"aspect_ratio_mask\"].shape)\nprint(\"cross_attention_mask shape: \\n\", model_inputs[\"cross_attention_mask\"].shape)\n# print(\"labels shape: \\n\", model_inputs[\"labels\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:48.779312Z","iopub.execute_input":"2025-03-14T16:03:48.779618Z","iopub.status.idle":"2025-03-14T16:03:48.899072Z","shell.execute_reply.started":"2025-03-14T16:03:48.779593Z","shell.execute_reply":"2025-03-14T16:03:48.898263Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\ninput_ids shape: \n torch.Size([1, 12])\nattention_mask shape: \n torch.Size([1, 12])\npixel_values shape: \n torch.Size([1, 1, 4, 3, 448, 448])\naspect_ratio_ids shape: \n torch.Size([1, 1])\naspect_ratio_mask shape: \n torch.Size([1, 1, 4])\ncross_attention_mask shape: \n torch.Size([1, 12, 1, 4])\n","output_type":"stream"}],"execution_count":13},{"id":"38d3fa62-dca0-472a-b6be-c78e72b73f80","cell_type":"code","source":"labels = processor(\n        images=None,\n        text=raw_dataset[\"prompt\"][0].replace(\"<|image|>\",\"\") + \" \" + raw_dataset[\"completion\"][0] ,\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    ).input_ids\nlabels[0,:11] = -100\nmodel_inputs[\"labels\"] = labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:50.759069Z","iopub.execute_input":"2025-03-14T16:03:50.759414Z","iopub.status.idle":"2025-03-14T16:03:50.783030Z","shell.execute_reply.started":"2025-03-14T16:03:50.759384Z","shell.execute_reply":"2025-03-14T16:03:50.782173Z"}},"outputs":[],"execution_count":14},{"id":"82067d5c-f4d2-42ea-b50d-09ee851f5471","cell_type":"code","source":"print(model_inputs.keys())\n# print(len(model_inputs.keys))\nprint(model_inputs[\"input_ids\"][0])\nprint(\"Pixel Values shape: \\n\",model_inputs[\"pixel_values\"].shape)  # Image shape\nprint(\"Input ids shape: \\n\",model_inputs[\"input_ids\"].shape)  # Tokenized text shape\nprint(\"attention mask shape: \\n\",model_inputs[\"attention_mask\"].shape)  # Attention mask shape\nprint(\"labels shape: \\n\",model_inputs[\"labels\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:53.062928Z","iopub.execute_input":"2025-03-14T16:03:53.063272Z","iopub.status.idle":"2025-03-14T16:03:53.071304Z","shell.execute_reply.started":"2025-03-14T16:03:53.063242Z","shell.execute_reply":"2025-03-14T16:03:53.070436Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask', 'labels'])\ntensor([128000, 128256, 128000,  30059,    682,   9621,   1495,    505,    420,\n          2217,  30357,     25])\nPixel Values shape: \n torch.Size([1, 1, 4, 3, 448, 448])\nInput ids shape: \n torch.Size([1, 12])\nattention mask shape: \n torch.Size([1, 12])\nlabels shape: \n torch.Size([1, 1128])\n","output_type":"stream"}],"execution_count":15},{"id":"bfb04538-06fe-4b69-ae58-6b1c92668794","cell_type":"code","source":"def preprocess_function(examples):\n    images = [Image.open(img_path).convert('RGB') for img_path in examples['image_path']]\n    \n    # For training, we need both inputs and labels\n    # For each example, we'll create:\n    # 1. Input: image + prompt\n    # 2. Labels: the full sequence (prompt + completion)\n    \n    # Process the input (image + prompt)\n    model_inputs = processor(\n        images=images,\n        text=examples['prompt'],\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    # For the labels, we need the full sequence (prompt + completion)\n    # We'll combine them here\n    full_texts = [prompt.replace(\"<|image|>\",\"\") + \" \" + completion \n                 for prompt, completion in zip(examples['prompt'], examples['completion'])]\n    for i, text in enumerate(full_texts) :\n        if \"<|image|>\" in (text):\n            print(f\"image token found in {i} \")\n    # Process the full text to get the labels\n    labels = processor(\n        images=None,\n        text=full_texts,\n        padding=\"longest\",\n        truncation=True,\n        return_tensors=\"pt\"\n    ).input_ids\n    \n    if labels.shape[0] != len(examples['prompt']):\n        raise ValueError(f\"Labels batch size {labels.shape[0]} doesn't match expected {len(examples['prompt'])}\")\n        \n    # Set -100 for the prompt portion (we don't want to calculate loss on that)\n    # This requires knowing the token length of each prompt\n    for i, prompt in enumerate(examples['prompt']):\n        # prompt_tokens = processor(images=None,text=prompt.replace(\"<|image|>\",\"\"), return_tensors=\"pt\").input_ids[0]\n        # prompt_length = len(prompt_tokens)\n        # Set all tokens before the completion to -100\n        labels[i, :11] = -100\n    \n    model_inputs[\"labels\"] = labels\n    \n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:55.476835Z","iopub.execute_input":"2025-03-14T16:03:55.477165Z","iopub.status.idle":"2025-03-14T16:03:55.483821Z","shell.execute_reply.started":"2025-03-14T16:03:55.477131Z","shell.execute_reply":"2025-03-14T16:03:55.482901Z"}},"outputs":[],"execution_count":16},{"id":"1e89194f-e733-469e-b4da-d0d6b4563c34","cell_type":"code","source":"processed_dataset = preprocess_function(raw_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:03:57.969077Z","iopub.execute_input":"2025-03-14T16:03:57.969473Z","iopub.status.idle":"2025-03-14T16:04:10.588941Z","shell.execute_reply.started":"2025-03-14T16:03:57.969442Z","shell.execute_reply":"2025-03-14T16:04:10.588261Z"}},"outputs":[],"execution_count":17},{"id":"5737041d-504f-496f-95c6-5d71a1d6e556","cell_type":"code","source":"print(\"Keys: \\n\", processed_dataset.keys())\nprint(\"input_ids shape: \\n\", processed_dataset[\"input_ids\"].shape)\nprint(\"attention_mask shape: \\n\", processed_dataset[\"attention_mask\"].shape)\nprint(\"pixel_values shape: \\n\", processed_dataset[\"pixel_values\"].shape)\nprint(\"aspect_ratio_ids shape: \\n\", processed_dataset[\"aspect_ratio_ids\"].shape)\nprint(\"aspect_ratio_mask shape: \\n\", processed_dataset[\"aspect_ratio_mask\"].shape)\nprint(\"cross_attention_mask shape: \\n\", processed_dataset[\"cross_attention_mask\"].shape)\nprint(\"labels shape: \\n\", processed_dataset[\"labels\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:10.589917Z","iopub.execute_input":"2025-03-14T16:04:10.590190Z","iopub.status.idle":"2025-03-14T16:04:10.598686Z","shell.execute_reply.started":"2025-03-14T16:04:10.590152Z","shell.execute_reply":"2025-03-14T16:04:10.597797Z"}},"outputs":[{"name":"stdout","text":"Keys: \n dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask', 'labels'])\ninput_ids shape: \n torch.Size([97, 12])\nattention_mask shape: \n torch.Size([97, 12])\npixel_values shape: \n torch.Size([1, 97, 4, 3, 448, 448])\naspect_ratio_ids shape: \n torch.Size([1, 97])\naspect_ratio_mask shape: \n torch.Size([1, 97, 4])\ncross_attention_mask shape: \n torch.Size([97, 12, 1, 4])\nlabels shape: \n torch.Size([97, 1624])\n","output_type":"stream"}],"execution_count":18},{"id":"3382ffac-94cb-44d5-8263-9d9c3eff41b1","cell_type":"code","source":"processed_dataset[\"pixel_values\"] = processed_dataset[\"pixel_values\"].permute(1, 0, 2, 3, 4, 5)  # Shape: [97, 1, 4, 3, 448, 448]\nprocessed_dataset[\"aspect_ratio_ids\"] = processed_dataset[\"aspect_ratio_ids\"].permute(1, 0)  # Shape: [97, 1]\nprocessed_dataset[\"aspect_ratio_mask\"] = processed_dataset[\"aspect_ratio_mask\"].permute(1, 0, 2)  # Shape: [97, 1, 4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:19.316450Z","iopub.execute_input":"2025-03-14T16:04:19.316824Z","iopub.status.idle":"2025-03-14T16:04:19.322495Z","shell.execute_reply.started":"2025-03-14T16:04:19.316794Z","shell.execute_reply":"2025-03-14T16:04:19.321347Z"}},"outputs":[],"execution_count":19},{"id":"9294ee67-4f00-4c8d-8e50-588cddc33b7d","cell_type":"code","source":"print(\"input_ids shape: \\n\", processed_dataset[\"input_ids\"].shape)\nprint(\"attention_mask shape: \\n\", processed_dataset[\"attention_mask\"].shape)\nprint(\"pixel_values shape: \\n\", processed_dataset[\"pixel_values\"].shape)\nprint(\"aspect_ratio_ids shape: \\n\", processed_dataset[\"aspect_ratio_ids\"].shape)\nprint(\"aspect_ratio_mask shape: \\n\", processed_dataset[\"aspect_ratio_mask\"].shape)\nprint(\"cross_attention_mask shape: \\n\", processed_dataset[\"cross_attention_mask\"].shape)\nprint(\"labels shape: \\n\", processed_dataset[\"labels\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:20.377135Z","iopub.execute_input":"2025-03-14T16:04:20.377487Z","iopub.status.idle":"2025-03-14T16:04:20.385822Z","shell.execute_reply.started":"2025-03-14T16:04:20.377463Z","shell.execute_reply":"2025-03-14T16:04:20.384970Z"}},"outputs":[{"name":"stdout","text":"input_ids shape: \n torch.Size([97, 12])\nattention_mask shape: \n torch.Size([97, 12])\npixel_values shape: \n torch.Size([97, 1, 4, 3, 448, 448])\naspect_ratio_ids shape: \n torch.Size([97, 1])\naspect_ratio_mask shape: \n torch.Size([97, 1, 4])\ncross_attention_mask shape: \n torch.Size([97, 12, 1, 4])\nlabels shape: \n torch.Size([97, 1624])\n","output_type":"stream"}],"execution_count":20},{"id":"625b61d5-e301-4814-b4cd-89f5a4a3b3b8","cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n# torch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:23.595630Z","iopub.execute_input":"2025-03-14T16:04:23.595982Z","iopub.status.idle":"2025-03-14T16:04:23.600070Z","shell.execute_reply.started":"2025-03-14T16:04:23.595954Z","shell.execute_reply":"2025-03-14T16:04:23.599253Z"}},"outputs":[],"execution_count":21},{"id":"b7515f42-9e4f-422c-8e08-25f2863e07d2","cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:26.184099Z","iopub.execute_input":"2025-03-14T16:04:26.184463Z","iopub.status.idle":"2025-03-14T16:04:26.188516Z","shell.execute_reply.started":"2025-03-14T16:04:26.184432Z","shell.execute_reply":"2025-03-14T16:04:26.187583Z"}},"outputs":[],"execution_count":22},{"id":"e1338b8e-61eb-401a-9d77-1dc7f3450a6a","cell_type":"code","source":"# # with torch.no_grad():\n# #     outputs = model(**processed_dataset)\n# #     print(\"Loss:\", outputs.loss.item())\n# with torch.no_grad():\n#  outputs = model(**processed_dataset)\n#  print(\"Loss:\", outputs.loss.item())\n\n# # This confirms the model can process your dataset format correctly\n# print(\"Forward pass successful! Your dataset format is correct for fine-tuning.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:29.082913Z","iopub.execute_input":"2025-03-14T16:04:29.083240Z","iopub.status.idle":"2025-03-14T16:04:29.087074Z","shell.execute_reply.started":"2025-03-14T16:04:29.083216Z","shell.execute_reply":"2025-03-14T16:04:29.086156Z"},"scrolled":true},"outputs":[],"execution_count":23},{"id":"ad0f872d-82a8-441b-a8dc-68abe78827ac","cell_type":"code","source":"print(type(processed_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:31.015927Z","iopub.execute_input":"2025-03-14T16:04:31.016280Z","iopub.status.idle":"2025-03-14T16:04:31.021794Z","shell.execute_reply.started":"2025-03-14T16:04:31.016254Z","shell.execute_reply":"2025-03-14T16:04:31.020675Z"}},"outputs":[{"name":"stdout","text":"<class 'transformers.feature_extraction_utils.BatchFeature'>\n","output_type":"stream"}],"execution_count":24},{"id":"a6353e06-b6f4-4d37-971c-ca3ea67f754a","cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, batch_feature):\n        self.data = batch_feature\n\n    def __len__(self):\n        return 97  # Fixed dataset length based on your input\n\n    def __getitem__(self, idx):\n        return {\n            key: torch.tensor(self.data[key][idx]) if isinstance(self.data[key], list) or isinstance(self.data[key], torch.Tensor)\n            else torch.tensor(self.data[key])  # Handle scalars\n            for key in self.data.keys()\n        }\n\n# Convert the dataset\ntrain_dataset = CustomDataset(processed_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:31.916687Z","iopub.execute_input":"2025-03-14T16:04:31.916987Z","iopub.status.idle":"2025-03-14T16:04:31.922878Z","shell.execute_reply.started":"2025-03-14T16:04:31.916965Z","shell.execute_reply":"2025-03-14T16:04:31.921820Z"}},"outputs":[],"execution_count":25},{"id":"161110a3-4b36-4109-86f5-1928285eac80","cell_type":"code","source":"print(type(train_dataset))\n# train_dataset[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:36.015052Z","iopub.execute_input":"2025-03-14T16:04:36.015409Z","iopub.status.idle":"2025-03-14T16:04:36.020382Z","shell.execute_reply.started":"2025-03-14T16:04:36.015382Z","shell.execute_reply":"2025-03-14T16:04:36.019313Z"}},"outputs":[{"name":"stdout","text":"<class '__main__.CustomDataset'>\n","output_type":"stream"}],"execution_count":26},{"id":"45810c07-72d8-4f20-984f-2687bcac7ce3","cell_type":"code","source":"# from datasets import load_dataset\n# from transformers import AutoTokenizer, DataCollatorWithPadding\n\n# raw_datasets = load_dataset(\"glue\", \"mrpc\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:38.148003Z","iopub.execute_input":"2025-03-14T16:04:38.148381Z","iopub.status.idle":"2025-03-14T16:04:38.152572Z","shell.execute_reply.started":"2025-03-14T16:04:38.148348Z","shell.execute_reply":"2025-03-14T16:04:38.151192Z"}},"outputs":[],"execution_count":27},{"id":"ce62bd2e-b154-46be-9514-470af6fbb85f","cell_type":"code","source":"# raw_datasets[\"train\"][3667]\n# print(type(raw_datasets[\"train\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:38.491465Z","iopub.execute_input":"2025-03-14T16:04:38.491827Z","iopub.status.idle":"2025-03-14T16:04:38.495610Z","shell.execute_reply.started":"2025-03-14T16:04:38.491799Z","shell.execute_reply":"2025-03-14T16:04:38.494540Z"}},"outputs":[],"execution_count":28},{"id":"8fa7f8bd-0393-45a4-b802-ebe06f223542","cell_type":"code","source":"type(processed_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:39.476283Z","iopub.execute_input":"2025-03-14T16:04:39.476640Z","iopub.status.idle":"2025-03-14T16:04:39.482252Z","shell.execute_reply.started":"2025-03-14T16:04:39.476614Z","shell.execute_reply":"2025-03-14T16:04:39.481392Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"transformers.feature_extraction_utils.BatchFeature"},"metadata":{}}],"execution_count":29},{"id":"4703a9ec-a2d4-4eb4-8037-1fcbddff4486","cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_dict(processed_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:42.045808Z","iopub.execute_input":"2025-03-14T16:04:42.046101Z","iopub.status.idle":"2025-03-14T16:04:54.318963Z","shell.execute_reply.started":"2025-03-14T16:04:42.046078Z","shell.execute_reply":"2025-03-14T16:04:54.318131Z"}},"outputs":[],"execution_count":30},{"id":"d88c3eb8-7209-4afe-aefa-0b810531b2cf","cell_type":"code","source":"processed_dataset = dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:54.320252Z","iopub.execute_input":"2025-03-14T16:04:54.320604Z","iopub.status.idle":"2025-03-14T16:04:54.324455Z","shell.execute_reply.started":"2025-03-14T16:04:54.320573Z","shell.execute_reply":"2025-03-14T16:04:54.323568Z"}},"outputs":[],"execution_count":31},{"id":"5c10e790-381e-4de8-88f9-03cb2d85112d","cell_type":"code","source":"processed_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:54.325604Z","iopub.execute_input":"2025-03-14T16:04:54.325826Z","iopub.status.idle":"2025-03-14T16:04:54.340048Z","shell.execute_reply.started":"2025-03-14T16:04:54.325806Z","shell.execute_reply":"2025-03-14T16:04:54.339091Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask', 'labels'],\n    num_rows: 97\n})"},"metadata":{}}],"execution_count":32},{"id":"ce15c078-e9b5-4539-9590-5cb4013ad5b1","cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"ðŸ”¥ CUDA is available! Checking memory usage...\")\n\n    # Total memory in the GPU\n    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n    # Currently allocated memory by PyTorch\n    allocated_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n\n    # Reserved memory by PyTorch (including cached memory)\n    reserved_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n\n    # Free memory available (approximation)\n    free_memory = total_memory - allocated_memory\n\n    print(f\"ðŸ›  Total GPU Memory: {total_memory:.2f} GB\")\n    print(f\"ðŸ’¾ Allocated Memory: {allocated_memory:.2f} GB\")\n    print(f\"ðŸ“¦ Reserved Memory: {reserved_memory:.2f} GB\")\n    print(f\"âœ… Free Memory: {free_memory:.2f} GB\")\nelse:\n    print(\"âŒ No CUDA device found. Running on CPU.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:04:59.277955Z","iopub.execute_input":"2025-03-14T16:04:59.278278Z","iopub.status.idle":"2025-03-14T16:04:59.288791Z","shell.execute_reply.started":"2025-03-14T16:04:59.278253Z","shell.execute_reply":"2025-03-14T16:04:59.287768Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¥ CUDA is available! Checking memory usage...\nðŸ›  Total GPU Memory: 14.74 GB\nðŸ’¾ Allocated Memory: 9.04 GB\nðŸ“¦ Reserved Memory: 9.23 GB\nâœ… Free Memory: 5.70 GB\n","output_type":"stream"}],"execution_count":33},{"id":"c256b14a-7e71-43eb-9b03-1d2da17b7911","cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import DataCollatorForSeq2Seq\nfrom transformers import default_data_collator\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./llama-3-vision-finetuned\",\n    per_device_train_batch_size=1,  # Adjust based on your GPU memory\n    gradient_accumulation_steps=1,  # Accumulate gradients to simulate larger batch\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    warmup_ratio=0.05,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    fp16=True,  # Use mixed precision\n    save_total_limit=2,\n    report_to=\"none\",  # Disable Wandb reports if not needed\n)\n\n# For memory efficiency, use PEFT (Parameter-Efficient Fine-Tuning)\n# This is especially important for large models like Llama 3.2\npeft_config = LoraConfig(\n    r=8,  # Rank\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],  # Common attention modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Prepare model for PEFT\nmodel = prepare_model_for_kbit_training(model)\npeft_model = get_peft_model(model, peft_config)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_dataset,\n    data_collator = default_data_collator\n)\n\n# Start training\ntrainer.train()\n\n# Save the model\npeft_model.save_pretrained(\"./llama-3-vision-finetuned-peft\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:11:51.261376Z","iopub.execute_input":"2025-03-14T16:11:51.261674Z","iopub.status.idle":"2025-03-14T16:11:51.324795Z","shell.execute_reply.started":"2025-03-14T16:11:51.261655Z","shell.execute_reply":"2025-03-14T16:11:51.323658Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-b52137cad680>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Prepare model for PEFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mpeft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             ) and param.__class__.__name__ != \"Params4bit\":\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     if (\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 104.12 MiB is free. Process 2952 has 14.64 GiB memory in use. Of the allocated memory 13.76 GiB is allocated by PyTorch, and 782.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 104.12 MiB is free. Process 2952 has 14.64 GiB memory in use. Of the allocated memory 13.76 GiB is allocated by PyTorch, and 782.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":38},{"id":"50cec221-ec8c-4cdc-9fd9-c609ca680e5d","cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:11:51.854172Z","iopub.execute_input":"2025-03-14T16:11:51.854482Z","iopub.status.idle":"2025-03-14T16:11:51.858617Z","shell.execute_reply.started":"2025-03-14T16:11:51.854457Z","shell.execute_reply":"2025-03-14T16:11:51.857681Z"}},"outputs":[],"execution_count":39},{"id":"d29944a9-1e4a-43c0-aee5-d362d2538fde","cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"ðŸ”¥ CUDA is available! Checking memory usage...\")\n\n    # Total memory in the GPU\n    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n    # Currently allocated memory by PyTorch\n    allocated_memory = torch.cuda.memory_allocated(0) / (1024 ** 3)  # Convert to GB\n\n    # Reserved memory by PyTorch (including cached memory)\n    reserved_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)  # Convert to GB\n\n    # Free memory available (approximation)\n    free_memory = total_memory - allocated_memory\n\n    print(f\"ðŸ›  Total GPU Memory: {total_memory:.2f} GB\")\n    print(f\"ðŸ’¾ Allocated Memory: {allocated_memory:.2f} GB\")\n    print(f\"ðŸ“¦ Reserved Memory: {reserved_memory:.2f} GB\")\n    print(f\"âœ… Free Memory: {free_memory:.2f} GB\")\nelse:\n    print(\"âŒ No CUDA device found. Running on CPU.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T16:13:48.567107Z","iopub.execute_input":"2025-03-14T16:13:48.567393Z","iopub.status.idle":"2025-03-14T16:13:48.575803Z","shell.execute_reply.started":"2025-03-14T16:13:48.567366Z","shell.execute_reply":"2025-03-14T16:13:48.574936Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¥ CUDA is available! Checking memory usage...\nðŸ›  Total GPU Memory: 14.74 GB\nðŸ’¾ Allocated Memory: 13.76 GB\nðŸ“¦ Reserved Memory: 14.52 GB\nâœ… Free Memory: 0.98 GB\n","output_type":"stream"}],"execution_count":42},{"id":"6d3eed09-fc33-4a65-a7dc-e6986ac4bc1b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}